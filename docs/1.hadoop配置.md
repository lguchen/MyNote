# hadoop-env.sh

```
export JAVA_HOME=/home/jdk
```

# core-site.xml

```
<configuration>
<property>
 <name>fs.defaultFS</name>
 <value>hdfs://192.168.106.140:9000</value>
</property>
<property>
 <name>io.file.buffer.size</name>
 <value>131072</value>
</property>
<property>
 <name>hadoop.tmp.dir</name>
 <value>file:/home/hadoop/tmp</value>
</property>
</configuration>
```

# hdfs-site.xml

```
<configuration>

<property>
 <name>dfs.namenode.name.dir</name>
 <value>file:/home/hadoop/dfs/name</value>
</property>
<property>
 <name>dfs.datanode.data.dir</name>
 <value>file:/home/hadoop/dfs/data</value>
 </property>
<property>
 <name>dfs.replication</name>
 <value>3</value>
</property>
</configuration>
```

# yarn-site.xml

```
<configuration>
<property>
 <name>yarn.resourcemanager.address</name>
 <value>master:8032</value>
</property>
<property>
 <name>yarn.resourcemanager.scheduler.address</name>
 <value>master:8030</value>
</property>
<property>
 <name>yarn.resourcemanager.address</name>
 <value>master:8032</value>
</property>
<property>
 <name>yarn.resourcemanager.scheduler.address</name>
 <value>master:8030</value>
</property>
<property>
 <name>yarn.resourcemanager.resource-tracker.address</name>
 <value>master:8031</value>
</property>
<property>
 <name>yarn.resourcemanager.admin.address</name>
 <value>master:8033</value>
</property>
<property>
 <name>yarn.resourcemanager.webapp.address</name>
 <value>master:8088</value>
</property>
<property>
 <name>yarn.nodemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
</property>
<property>
 <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
 <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
</configuration>
```

## spark yarn-site.xml

```
<configuration>
<property>
<name>yarn.resourcemanager.hostname</name>
<value>master</value>
</property>
<property>
<name>yarn.nodename.sux.services</name>
<value>mapreduce_shuffle</value>
</property>
</configuration>

```

# mapred-site.xml

```
<configuration>
<property>
 <name>mapreduce.framework.name</name>
 <value>yarn</value>
</property>
<property>
 <name>mapreduce.jobhistory.address</name>
 <value>master:10020</value>
</property>
<property>
 <name>mapreduce.jobhistory.webapp.address</name>
 <value>master:19888</value>
</property>
</configuration>
```

# slaves

```
slave1
slave2
```

# spark 的 yarn-site.xml

```
<configuration>
<property>
 <name>mapreduce.framework.name</name>
 <value>yarn</value>
</property>
# 指定yarn的主机
<property>
 <name>yarn.resourcemanager.hostname</name>
 <value>master</value>
</property>
# 指定hadoop架包在yarn上运行的支持方式
<property>
 <name>yarn.nodemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
</property>
# 设置yarn的内存分配方案，MB为单位
<property>
 <name>yarn.nodemanager.resource.memory-mb</name>
 <value>1024</value>
</property>
<property>
 <name>yarn.scheduler.minimun-allocation-mb</name>
 <value>1024</value>
</property>
<property>
 <name>yarn.nodemanager.vmem-pmem-ration</name>
 <value>2.1</value>
</property>
# 开启日志聚合功能
<property>
 <name>yarn.log-aggregation-enable</name>
 <value>true</value>
</property>
# 设置聚合日志在hdfs上的保存时间
<property>
 <name>yarn.log-aggregation.retain-seconds</name>
 <value>604800</value>
</property>
# 设置yarn的历史服务器
<property>
 <name>yarn.log.server.url</name>
 <value>http://master:19888/jobhistory/logs</value>
</property>
# 关闭yarn的内存检查
<property>
 <name>yarn.nodemanager.pmem-check-enabled</name>
 <value>false</value>
</property>
<property>
 <name>yarn.nodemanager.vmem-check-enabled</name>
 <value>false</value>
</property>
</configuration>
```

# 首次启动前格式化 namenode

```bash
hadoop namenode -format
```
