### 解压、改名、配置环境变量   同hadoop，这里和后面的都不再重复写出，直接到配置文件

**1、修改配置文件**

```bash
cd /opt/modules/hive/conf
```

**(1)、编辑 hive-env.sh**

```bash
cp hive-env.sh.template hive-env.sh
vi hive-env.sh
```
**修改hadoop安装位置和hive配置文件所在位置**

修改内容如下：

```sh
export HADOOP_HOME=/opt/modules/hadoop
export HIVE_CONF_DIR=/opt/modules/hive/conf
```


**(2)、编辑 hive-site.xml**

这里本身没有这个文件，需要自己创建并编辑

直接输入如下命令：

```bash
vi hive-site.xml
```

文件内容如下：

```xml
<configuration>
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false&amp;charset=utf8</value> #（mysql地址localhost）
        </property>
        <property>
                <name>javax.jdo.option.ConnectionDriverName</name> #（mysql的驱动）
                <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionUserName</name> # mysql 用户名
                <value>root</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionPassword</name> # mysql 密码
                <value>root</value>
        </property>
        <property>
                <name>hive.metastore.schema.verification</name>
                <value>false</value>
        </property>
</configuration>
```

**(3)、配置环境变量**

```sh
export HIVE_HOME=/opt/modules/hive
export PATH=$PATH:$HIVE_HOME/bin
```
使用命令`source /etc/profile`使环境变量生效

**2、拷贝MySQL JDBC驱动程序**

将 mysql-connector-java-5.1.40.bin.jar 拷贝到 $HIVE_HOME/lib

```bash
cp /opt/modules/software/mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar /opt/modules/hive/lib/
```

**3、删除Hadoop中旧版的 jline-0.9.94.jar**

```
rf -rf /opt/modules/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar
```

**4、将hive/lib目录下的  jline-2.12.jar复制进去**

```bash
cp /opt/modules/hive/lib/jline-2.12.jar /opt/modules/hadoop/share/hadoop/yarn/lib/
```

**5、初始化hive数据库**

```bash
schematool -dbType mysql -initSchema
```

初始化成功如下所示：

![image-20230402153547331](img\hive_init.png)

最后先启动hadoop再启动hive

```bash
start-all.sh  #启动hadoop
hive   #启动hive
```

