# 解压改名
```sh
tar -xzf /opt/software/hbase-1.2.0-bin.tar.gz -C /opt/modules/
mv /opt/modules/hbase-1.2.0/ /opt/modules/hbase
```
# 修改配置文件
进入HBase安装目录的conf目录下修改配置文件

1. hbase-site.xml 

```xml
<configuration>
        <property>
                <name>hbase.rootdir</name>
                <value>hdfs://master:9000/hbase</value>
        </property>
        <property>
                <name>hbase.cluster.distributed</name>
                <value>true</value>
        </property>
        <property>
		<name>hbase.zookeeper.quorum</name>
		<value>master,slave1,slave2</value>
	</property>
        <property>
	        <name>hbase.zookeeper.property.clientPort</name>
		<value>2181</value>
	</property>
</configuration>
```
说明：
- hbase.rootdir：配置HBase数据存放目录，这里使用的是HDFS，这个值的地址和端口需要与安装的Hadoop目录下etc/hadoop/core-site.xml文件中的fsdefaultFS属性值相同，即core-site.xml的值设置为hdfs://master:8020/，则hbase.rootdir的值设置为hdfs://master:8020/habse

- hbase.cluster.distributed：默认为false，表示单机运行，true表示在分布模式下运行

- hbase.zookeeper.quorum：该属性配置的是ZooKeeper集群个服务器的位置，一般为奇数个服务器

- hbase.zookeeper.property.clientPort：该属性配置的是ZooKeeper的端口号，这个属性和ZooKeeper安装目录下conf/zoo.cfg文件中的clientPort属性值一样

2. hbase-env.sh
```sh
# 在底部添加如下内容
export JAVA_HOME=/opt/modules/jdk
export HBASE_CLASSPATH=/opt/modules/hbase/conf
export HBASE_MANAGES_ZK=false
```
说明： HBASE_MANAGES_ZK 这个配置是用来指定 HBase 是否管理 ZooKeeper 的选项，true表示使用HBase自带的ZooKeeper，false代表使用外部安装的ZooKeeper,即自己安装的Zookeeper

3. regionservers
```sh
# 删除localhost，修改成如下内容
master
slave1
slave2
```

# 将hbase分发到另外两台机器
```sh
scp -r /opt/modules/hbase/ root@slave1:/opt/modules/
scp -r /opt/modules/hbase/ root@slave2:/opt/modules/
```

# 配置环境变量，三台机器都需要配置
```sh
vi /etc/profile
# 添加如下内容
export HBASE_HOME=/opt/modules/hbase
export PATH=$PATH:$HBASE_HOME/bin
```
配置好后使用命令 ` source /etc/profile ` 使环境变量生效

# 启动Hbase
<mark>启动Hbase前需要先启动hadoop集群和zookeeper</mark>

1. 启动hadoop集群

使用命令 ` start-all.sh `

2. 分别在三台机器启动zookeeper
```sh
master:
[root@master ~]# cd /opt/modules/zookeeper/bin
[root@master bin]# ./zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/modules/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED

slave1:
[root@slave1 ~]# cd /opt/modules/zookeeper/bin
[root@slave1 bin]# ./zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/modules/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED

slave2:
[root@slave2 ~]# cd /opt/modules/zookeeper/bin
[root@slave2 bin]# ./zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/modules/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
```
3. 使用命令输入以下命令启动 hbase

` start-hbase.sh `
输入`jps`查看进程，进程如下所示：
![进程](img/hbase%E5%90%AF%E5%8A%A8.png)

出现`Hmaster`和`HRegionServer`进程表示启动成功

4. 进入Shell界面，输入命令`hbase shell`


# 配置HBase集群的高可用
意义或者是作用：
在HBase中HMaster负责监控HRegionServer的生命周期，均衡RegionServer的负载，如果HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置。

1. 关闭Hbase集群，没有开启不用管，
命令`stop-hbase.sh`

2. 在conf目录下创建backup-masters文件

    `vi /opt/modules/hbase/conf/backup-masters`

    输入内容：slave1
    - - -
    注意：这个备份节点可以设置多个，且可以任意设置
    - - -
3. 分发到集群的其他节点，命令如下

    ```
    scp /opt/modules/hbase/conf/backup-masters root@slave1:/opt/modules/hbase/conf

    scp /opt/modules/hbase/conf/backup-masters root@slave2:/opt/modules/hbase/conf
    ```