# 前言，必读

****

本次环境搭建默认每个模式都是分开搭建，**总共5台机器**，若配置伪分布式之后想要此台机器继续**配置全分布式**，**请勿进行格式化和启动集群的操作**，配置完全分布式后再进行格式化操作并启动集群，**若已进行格式化操作**，请参照网上教程删除相关文件后再继续配置全分布式。[点击这里跳转全分布式](#全分布式:三台服务器(master、slave1、slave2))



# 安装jdk环境

- 解压安装包，这里用的**版本是jdk1.8**

  ```bash
  tar -xzvf jdk-8u181-linux-x64.tar.gz -C /opt/modules
  ```

- 为了方便简洁，将文件夹改名为jdk

  ```bash
  mv /opt/modules/jdk1.8.0_181 /opt/modules/jdk
  ```

- 配置环境变量，命令：vi /etc/profile    内容如下

  ```bash
  export JAVA_HOME=/opt/modules/jdk
  export PATH=$PATH:$JAVA_HOME/bin
  export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
  ```
使用命令`source /etc/profile`使环境变量生效

# 安装hadoop，版本2.6.0

- 同上，解压改名

  ```bash
  tar -xzvf hadoop-2.6.0.tar.gz -C /usr/local/src
  mv /opt/modules/hadoop-2.6.0 /opt/modules/hadoop
  ```

- 配置环境变量，命令：vi /etc/profile    内容如下

  ```bash
  export HADOOP_HOME=/opt/modules/hadoop
  export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
  ```

# 相关配置

## 本地模式 ：一台服务器(master)

本地模式没有[HDFS](https://blog.csdn.net/sjmz30071360/article/details/79877846)，只能测试MapReduce程序，程序运行的结果保存在本地文件系统。

**配置好环境变量即是本地模式**

Hadoop中为我们提供了一个单词计数的MapReduce程序 **hadoop-mapreduce-examples-2.6.0.jar**， **在 hadoop/share/hadoop/mapreduce**目录下

请自行测试

## 伪分布式：一台服务器(master)

伪分布式模式在单机上运行，模拟全分布式环境，具有Hadoop的主要功能。它在本地模式基础之上，再如下修改配置文件即可。具体配置如下：

1. hdfs-site.xml

```xml
<configuration>
        <property>
                <!-- HDFS数据冗余度，默认3  -->
                <name>dfs.replication</name>
                <value>1</value>
        </property>
        <property>
                <!-- 是否开启HDFS权限检查，默认true  -->
                <name>dfs.permissions</name>
                <value>false</value>
        </property>
</configuration>
```

参数说明：

（1）dfs.replication

**配置数据的副本数。因为这里是单机，所以副本数配置为1。**

（2）dfs.permissions

**配置HDFS的权限检查。默认是true，也就是开启权限检查。可以不配置，这里只是为了说明。**



2. core-site.xml

```xml
<configuration>
        <property>
                <!-- 配置NameNode地址  -->
                <name>fs.defaultFS</name>
                <value>hdfs://master:9000</value>
        </property>
        <property>
                <!-- 保存HDFS临时数据的目录  -->
                <name>hadoop.tmp.dir</name>
                <value>/opt/modules/hadoop/tmp</value>
        </property>
</configuration>
```

参数说明：

（1）fs.defaultFS

**配置NameNode的地址，通信端口号是9000。master为主机名，也可以使用IP地址。**

（2）hadoop.tmp.dir

**配置HDFS数据保存目录，默认是Linux系统的tmp目录，而Linux系统tmp目录重启后会被删除，所以这里需要配置为本地系统的其他目录，例如Hadoop安装目录下的tmp目录。tmp目录需要用户自己创建**



3. mapred-site.xml

Hadoop配置文件中默认没有这个文件，只提供了模板文件mapred-site.xml.template，需要在当前目录下复制一份：

```bash
cp mapred-site.xml.template mapred-site.xml
```

具体配置内容如下：

```xml
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
</configuration>
```

参数说明：

（1）mapreduce.framework.name

**配置mapreduce程序执行的框架名称：yarn。yarn是资源管理器框架。**

4. yarn-site.xml

```xml
<configuration>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>master</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
</configuration>
```

参数说明：

（1）yarn.resourcemanager.hostname

**配置yarn的主节点ResourceManager主机名；**

（2）yarn.nodemanager.aux-services

**配置yarn的NodeManager运行MapReduce的方式。**

5. hadoop-env.sh

   ```sh
   JAVA_HOME=/usr/local/src/jdk
   ```

6. 格式化**Namenode**，若是要继续搭建[全分布式](#全分布式三台服务器masterslave1slave2)则无需格式化，也无需启动集群 直接跳到 [全分布式](#全分布式三台服务器masterslave1slave2)

执行如下命令

```bash
hdfs namenode -format #格式化Namenode
```

成功日志显示：

```
 ......
 INFO util.ExitUtil: Exiting with status 0
```

7. 启动伪分布式

（1）start-all.sh

​    执行start-all.sh（或者可以分别执行start-dfs.sh和start-yarn.sh命令）命令，正常启动后如下：

master节点服务如下：

a. NameNode

b. DataNode

c. SecondaryNameNode

d. ResourceManager

e. NodeManager

  (2)浏览器UI

首先执行如下命令关闭防火墙并禁止服务自启动，**必须先关闭防火墙**：  

```bash
#关闭防火墙
systemctl stop firewalld
#禁止自启动
systemctl disable firewalld
```

在浏览器中输入地址[http://192.168.80.10:50070](http://192.168.80.10:50070/)（SecondaryNameNode端口默认是50090），即可打开Hadoop管理页面

输入[http://192.168.80.10:8088/cluster](http://192.168.80.10:8088/cluster)，打开yarn应用管理页面



## 全分布式:三台服务器(master、slave1、slave2)

1. 编辑hosts文件 ,**三台服务器都要配置**

执行  vi /etc/hosts 打开主机名配置文件，配置主机名和IP地址对应关系如下：

   ```bash
   192.168.80.10 master
   192.168.80.11 slave1
   192.168.80.12 slave2
   ```

2. 免密码登录

（1）生成公钥和私钥

**三台服务器**分别执行命令  ssh-keygen -t rsa  生成公钥和私钥，过程按Enter即可

（2）拷贝公钥

将每台机器的公钥拷贝到其他机器（包括自己）
执行如下命令

```bash
# 将公钥复制到每台机器
ssh-copy-id localhost   # 提示yes or no,输入yes即可，然后再是输入密码
ssh-copy-id root@slave1
ssh-copy-id root@slave2
```

说明：`ssh-copy-id`命令会在 `~/.ssh/`目录下生成`authorized_keys`文件，此文件正是存放公钥实现免密的关键

3. 修改配置文件

（1）hadoop-env.sh

**在本地模式下，可以不配置JDK路径，但是全分布式模式必须要配置，不然启动集群时，会报错**

   ```sh
   JAVA_HOME=/opt/modules/jdk
   ```

   （2）hdfs-site.xml

   ```xml
   <configuration>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
   	        <value>master:50090</value>
        </property>
        <property>
                <!-- HDFS数据冗余度，默认3  -->
                <name>dfs.replication</name>
                <value>2</value>
        </property>
        <property>
                <!-- 是否开启HDFS权限检查，默认true  -->
                <name>dfs.permissions</name>
                <value>false</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/opt/modules/hadoop/dfs/name</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/opt/modules/hadoop/dfs/data</value>
        </property>
   </configuration>
   ```

   数据副本数配置为2，默认是3，这里只有两个datanode，所以配置为2。

   （3）core-site.xml

   ```xml
   <configuration>
        <property>
                <!-- 配置NameNode地址  -->
                <name>fs.defaultFS</name>
                <value>hdfs://master:9000</value>
        </property>
        <property>
                <!-- 保存HDFS临时数据的目录  -->
                <name>hadoop.tmp.dir</name>
                <value>/opt/modules/hadoop/tmp</value>
        </property>
        <property>
                <name>io.file.buffer.size</name>
                <value>131072</value>
                <description>该属性值单位为KB，131072KB即为默认的 64M</description>
        </property>
   </configuration>
   ```

   NameNode地址为master。

   （4）yarn-site.xml

   ```xml
   <configuration>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>master</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
        <property>
   		<name>yarn.resourcemanager.scheduler.address</name>
   		<value>master:8030</value>
        </property>
        <property>
   	        <name>yarn.resourcemanager.resource-tracker.address</name>
   	        <value>master:8031</value>
        </property>
        <property>
                <name>yarn.resourcemanager.address</name>
                <value>master:8032</value>
        </property>
        <property>
                <name>yarn.resourcemanager.admin.address</name>
                <value>master:8033</value>
        </property>
        <property>
   		<name>yarn.resourcemanager.webapp.address</name>
   		<value>master:8088</value>
   	</property>
        <!-- 关闭物理和虚拟内存检查  -->
        <property>
           	<name>yarn.nodemanager.pmem-check-enabled</name>
           	<value>false</value>
        </property>
        <property>
           	<name>yarn.nodemanager.vmem-check-enabled</name>
            	<value>false</value>
        </property>
   </configuration>
   ```
   ResourceManager节点主机名为master。

   （5）mapred-site.xml

   ```xml
   <configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <!-- 历史服务，按需配置  -->
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>master:10020</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>master:19888</value>
        </property>
   </configuration>
   ```

   （6）slaves

salves配置文件内容如下：
   ```
   slave1
   slave2
   ```

4. 其他节点配置

将master的hadoop目录远程拷贝到slave1和slave2上，命令如下：

   ```sh
   scp -r /opt/modules/hadoop root@slave1:/opt/modules
   scp -r /opt/modules/hadoop root@slave2:/opt/modules
   ```

此时相关的配置也一同被复制过去

5. 格式化NameNode

    命令如下

   ```sh
   hdfs namenode -format
   ```

6. 启动集群 start-all.sh  

   jps 查看进程，进程分别如下

   ```
   ------master------
   5762 Jps
   5302 ResourceManager
   4984 NameNode
   5160 SecondaryNameNode
   
   ------slave1------
   2677 DataNode
   2777 NodeManager
   3018 Jps
   
   ------slave2------
   2532 DataNode
   2869 Jps
   2632 NodeManager
   ```
