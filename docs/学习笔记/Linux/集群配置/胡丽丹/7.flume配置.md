# flume-env.sh
`export JAVA_HOME=/home/jdk`

## 在flume目录下编辑文件simple-hdfs-flume.conf
```bash
a1.sources=r1
a1.sinks=k1
a1.channels=c1
a1.sources.r1.type=spooldir
a1.sources.r1.spoolDir=/home/hadoop/logs/
a1.sources.r1.fileHeader=true
a1.sinks.k1.type=hdfs
a1.sinks.k1.hdfs.path=hdfs://master:9000/tmp/flume
a1.sinks.k1.hdfs.rollsize=1048760
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.rollInterval=900
a1.sinks.k1.hdfs.useLocalTimeStamp=true
a1.channels.c1.type=file
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

## 查看版本号`flume-ng version`

### 使用 `flume-ng agent` 命令加载 simple-hdfs-flume.conf 配置信息，启 动 flume 传输数据。

`flume-ng agent --conf-file simple-hdfs-flume.conf --name a1`

### 查看 Flume 传输到 HDFS 的文件，若能查看到 HDFS 上/tmp/flume 目 录有传输的数据文件，则表示数据传输成功
`hdfs dfs -ls /tmp/f1ume`
